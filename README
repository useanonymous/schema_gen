This is the code for our story-level model.

requirements
-python==3.7
-torch==1.12
-transformers==4.20.1
-sentence-transformers

# Dataset already contains the schema
# If you want to get the candidate knowledge set, please go to "https://github.com/allenai/comet-atomic-2020/" get the details

#Training:
Train pg module:
python pg_train.py --datapath dataset.jsonl --pretrain_model facebook/bart-large --batch_size 32 --save_path pg_save --learning_rate 1e-5 --save_model --num_of_knows 60

Train sr module:
python sr_train.py --datapath dataset.jsonl --pretrain_model gpt2 --batch_size 32 --save_path sr_save --learning_rate 1e-5 --save_model

#Generation:
Generate plot:
python pg_generate.py --datapath dataset.jsonl --checkpoint_path pg_save/model/ --max_gen_length 200 --batch_size 16 --top_p 0.9 --top_k 0 --temperature 1 --output_path pg_output.jsonl --num_of_knows 60 --generate

Data_processing:
python data_processing.py --pg_output pg_output.jsonl --sr_input sr_input.jsonl

Story generation:
python sr_generate.py --checkpoint_path sr_save/model/ --top_p 0.9 --top_k 0 --temperature 1 --input_path sr_input.jsonl --output_path sr_output.jsonl

#######
"schema_get_function.py" contains the function of schema get introduced in SA module.


"sample.txt" is 100 samples generated by our story-level model.